\section{Automatiserade tester av webbapplikationer. - Erik Malmberg}
\subsection{Inledning}
Den här enskilda utredningen är en del av kandidatrapporten i kursen TDDD77 vid Linköpings universitet.
Utredningen behandlar en del av utvecklingen av ett webbaserat system för att underlätta förberedelser
inför operationer på sjukhusen i Östergötland. Systemet utvecklades på uppdrag av Region Östergötland.

\subsubsection{Syfte}
Syftet med den här enskilda delen av kandidatarbetet är att ge insikt 
i hur kontinuerlig integration och 
automatiserade tester kan användas för att effektivisera testandet 
i ett projekt som använder en agil 
utvecklingsmetod. Speciellt ska det undersökas hur väl det går att använda 
webbaserade tjänster för
att utföra kontinuerliga automatiserade tester av webbapplikationer.

\subsubsection{Frågeställning}
De frågeställningar som ska besvaras i den här enskilda 
delen av rapporten är:

\begin{itemize}
\item Hur kan man använda webbaserade tjänster för
att utföra kontinuerliga automatiserade tester av webbapplikationer?
\item Hur effektivt är det att använda en webbaserad tjänst
för automatiserade tester? 
\item Vilka typer av tester är svåra att utföra med en sådan tjänst?
\end{itemize}

I den andra frågeställningen så definieras effektivitet som antalet test som kan
utföras per sekund. I svaret på frågeställningen ska även testfallen 
specificeras noggrant så att svaret inte blir tvetydigt. Det kan även vara
intressant att ta reda på hur lång tid det tar från det att koden läggs 
upp på GitHub
till det att testfallen börjar köras på servern.

\subsubsection{Avgränsningar}
Inga undersökningar kommer att utföras om hur andra lösningar än 
Travis CI kan användas för kontinuerlig 
integration och automatiserde tester. De testfall som kommer användas 
kommer uteslutande att vara skrivna med ramverket Jasmine. Den webbapplikation
som kommer att testas kommer att vara skriven med programmeringsspråket
Javascript och använda Javascriptbiblioteken Node.js och jQuery.

\subsection{Bakgrund}
Här beskrivs de tjänster, språk och bibliotek som använts under arbetet
med den här enskilda rapporten.

\subsubsection{Travis CI}
Travis CI är en webbaserad tjänst för att köra automatiserade enhetstester och integrationstester
på projekt som finns på GitHub. Travis CI är gratis att använda och byggt på öppen källkod
som är tillgänglig under en MIT-licens. 
Tjänsten har stöd för många olika programmeringsspråk, men det som är 
relevant för innehållet i den här rapporten
är Javascript med Node.js. För att konfigurera Travis CI används filen .travis.yml 
som placeras i det aktuella
projektets repository på GitHub.
Travis CI kör automatiskt de testfall som specificerats av användaren 
när kod läggs upp på GitHub.

\subsubsection{Javascript}
Javascript är ett programmeringsspråk som i första hand används på klientsidan på webbsidor.
Javascript exekveras av webbläsaren och arbetar mot ett gränssnitt som heter 
Document Object Model (DOM).

\subsubsection{JQuery}
JQuery är ett Javascriptbibliotek som kan användas för att förenkla programeringen
av Javascript på klientsidan av en webbsida. JQuery innehåller lättanvänd
funktionalitet för händelsehantering och modifiering av HTML-objekt.
JQuery är gratis och baserat på öppen källkod som är tillgänglig under en
MIT-licens.

\subsubsection{Node.js}
För information om Node.js se rubriken \emph{2.1.1 Node.js}
i det gemensamma avsnittet \emph{2.1 Programspråk och bibliotek}.

\subsubsection{Jasmine}
Jasmine är ett ramverk för testning av Javascript. 
Den node-modul som används är grunt-contrib-jasmine som använder task runnern Grunt 
för att köra testfall som skrivits med Jasmine.
Grunt kunfigureras med filen Gruntfile.js. Jasmine är baserat på öppen källkod
som är tillgänglig under en MIT-licens.

\subsection{Teori}
Här beskrivs den teori som ligger till grund för rapporten och som visar
varför frågeställningarna är relevanta.

\subsubsection{Vattenfallsmodellen}
I vattenfallsmodellen genomförs all integration och alla tester efter att implementeringen är slutförd. 
Om ett problem då identifieras under integrationen så är det krångligt att gå 
tillbaka och åtgärda problemet, vilket 
kan leda till förseningar av projektet.
Om felet som upptäcks är så allvarligt att en betydande omdesign måste ske så
kommer utvecklingen i stort sett att börja om från början och man kan räkna 
med en hundraprocentig ökning av budgeten, 
både vad gäller pengar och tid \cite{Royce}.

\subsubsection{Kontinuerlig integration och automatiserade tester}
Kontinuerlig integration kan leda till att problemen identifieras tidigare i 
utvecklingsprocessen. Problemen blir då lättare att åtgärda. Automatiserade tester kan effektivisera 
testprocessen och det finns många tillgängliga lösningar för att köra automatiserade
tester \cite{Karlsson}.
Några av de vanligaste är Travis CI, Codeship och Drone.io.

\subsection{Metod}
Här beskrivs den metod som användes för att 
besvara frågeställningarna.

Arbetet inleddes med att en teoriundersökning genomfördes
angående vilka metoder, ramverk och tjänster som skulle kunna
användas för att genomföra testerna i projektet.

De ramverk för
testning som undersöktes var Jasmine, Qunit och Mocha. Det
ramverk som valdes för arbetet med projektet var Jasmine
eftersom det är enkelt att konfigurera och har ett 
tydligt och intuitivt syntax. Att Jasmine var enkelt och tydligt
var viktigt eftersom det var många andra uppgifter förutom
testning som skulle utföras i projektet och tiden för
arbetet med projektet var begränsad till 300 timmar. Det var
alltså ingen i projektgruppen som hade tid att lära sig 
en komplicerad syntax i ett ramverk även om det varit
mer kraftfullt.

De tjänster för automatiserade tester som undersöktes var
Travis CI, Codeship och Drone.io. Även här 
så gjordes valet beroende på hur enkla tjänsterna var att
konfigurera och använda. Den tjänst som verkade enklast
och således valdes var Travis CI.

Arbetet med Travis CI inleddes med att tjänsten kopplades till 
projektets repository på GitHub. Kopplingen utfördes
genom att administratören för repositoryn loggade in på travis-ci.org med 
sitt GitHub-konto och aktiverade
en webhook för repositoryn.

Inställningarna för Travis CI konfigurerades med filen .travis.yml i projektets
repository. Språket valdes till
Javascript med Node.js med inställningen: \emph{language: node\textunderscore js}.
Versionen av Node.js valdes till version 0.10
med inställningen: \emph{node\textunderscore js: "0.10"}.

De nödvändiga node-modulerna installerades med hjälp av node package manager (npm).
Grunt installerades
med kommandot: \emph{npm install -g grunt-cli}. Grunt-contrib-jasmine installerades med kommandot: 
\emph{npm install grunt-contrib-jasmine}.

Task runnern Grunt konfigurerades med filen Gruntfile.js i projektets repository.
En task för Jasmine laddades in med
inställningen: \emph{grunt.loadNpmTasks('grunt-contrib-jasmine');}.
Tasken konfigurerades med följande kod i Gruntfile.js.

\begin{lstlisting}[
  basicstyle = \small
]
module.exports = function(grunt) {

  grunt.initConfig({
    pkg: grunt.file.readJSON('package.json'),
    jasmine: {
      test: {
        src: './public/js/*.js',
	options: {
	  vendor: [
	    'public/js/lib/jquery/jquery-2.1.1.js',
	    'node_modules/jasmine-jquery/lib/jasmine-jquery.js'
          ],
	  keepRunner: true,
	  specs: 'test/*-spec.js',
	  template: 'test/template/spec-template.tmpl'
        }
      }
    }
  });

  grunt.loadNpmTasks('grunt-contrib-jasmine');
}
\end{lstlisting}

Med \emph{src: './public/js/*.js'} valdes de filer som som skulle testas.
Med vendor valdes andra filer som var nödvändiga för att köra testerna.
Raden \emph{keepRunner: true} gör att filen \textunderscore SpecRunner.html sparas efter att
testerna körts. Filen kan sedan öppnas i en webbläsare och innehåller
detaljerad information om utfallet av testerna.
Med \emph{specs: 'test/*-spec.js'} valdes de testfall som skulle köras.
Alla filer som slutar med -spec.js i mappen test anses alltså vara
testfall som ska köras.
Raden \emph{template: 'test/template/spec-template.tmpl'} gör att testerna
körs med en speciell SpecRunner som även kan innehålla HTML-kod som testfallen
kan modifiera.
Eftersom Travis CI använder npm för att starta 
testerna så definierades testskriptet för npm med raden
\emph{''test'': ''grunt jasmine --verbose''} i filen package.json 
i projektets repository.

Testfallen skrevs med Jasmine. Jasmine har en enkel och intuitiv syntax.
Ett exempel på ett testfall skrivet med Jasmine följer nedan.

\begin{lstlisting}[
  basicstyle = \small
]
describe('The function splitOnce', function() {
	
  it('can split a string with a char correctly', function() {
    var str = 'a.b.c.d';
    var res = splitOnce(str, '.');
    expect(res[0]).toBe('a');
    expect(res[1]).toBe('b.c.d');
  });
}
\end{lstlisting}

På den första raden beskrivs vilken del av koden det är som ska testas.
På nästa rad beskrivs vad det är som ska testas i den utvalda delen av koden.
Med funktionen expect kontrolleras att koden har utfört testet på det sätt
som förväntats. Flera expect-funktioner kan användas i samma testfall.

Ett speciellt testfall skrevs
för att besvara den andra frågeställningen om hur effektivt det är 
att använda en webbaserad tjänst
för automatiserade tester. Det speciella testfallet visas nedan.

\begin{lstlisting}[
  basicstyle = \small
]
describe('Travis CI', function() {
	
  it('can do a lot of tests', function() {
    var date = new Date();
    var startTime = date.getTime();
    var time = startTime;
    var i = 0;

    while (time < startTime + 1000) {  
      var str = 'a.b.c.d';
      var res = splitOnce(str, '.');
      expect(res[0]).toBe('a');
      expect(res[1]).toBe('b.c.d');

      i++;
      date = new Date(); 
      time = date.getTime();
    };

    console.log(i);
  });
});
\end{lstlisting}

Testfallet testar funktionen splitOnce så många gånger som möjligt
under en sekund. Antalet gånger som funktionen hann köras skrivs
sedan ut på skärmen med en console.log. 
Testfallet har körts flera gånger på olika datum och olika tidpunkter.
Resultatet av testerna presenteras i \emph{Tabell E.5.1}
under rubriken \emph{E.5 Resultat}.
För att antalet ska
få en konkret betydelse visas även funktionen splitOnce nedan.

\begin{lstlisting}[
  basicstyle = \small
]
var splitOnce = function(str, split) {
  var index = str.indexOf(split);
  if (index === -1) {
    return [str, ''];
  }

  return [str.slice(0,index), str.slice(index + 1)];
};
\end{lstlisting}

Funktionen tar två paramterar. En sträng (str) som ska delas upp och 
en sträng (split) som anger vilket tecken eller vilken teckenkombination
som ska dela upp strängen. Funktionen delar endast upp strängen i två delar
även om (split) förekommer på flera positioner i (str). Funktionen returnerar 
en array med två element.
De två elementen är de två delarna av den ursprungliga strängen. Om den andra
parametern (split) inte existerar i den första parametern (str) så returneras
hela strängen i det första elementet och en tom sträng i det andra elementet.

För att besvara den tredje frågeställningen om vilka typer av tester som är 
svåra att utföra med en webbaserad tjänst så gicks koden igenom och 
försök till att skriva testfall genomfördes
med de olika delarna av koden. Resultatet av dessa tester redovisas under 
avsnittet \emph{E.5 Resultat}.

\subsection{Resultat}
Här presenteras resultatet av rapporten.

Resultatet av testerna som utfördes för att besvara den andra frågeställningen
visas i \emph{Tabell E.5.1}.

\noindent\emph{Tabell E.5.1}
\begin{center}
  \begin{tabular}{| l | l | l | l |}
  \hline
  Testnummer & Datum (ÅÅ-MM-DD) & Tid (hh-mm-ss) & Tester per sekund\\ \hline
  1 & 15-05-01 & 13:53:34 & 11380\\ \hline
  2 & 15-05-01 & 14:27:27 & 12462\\ \hline
  3 & 15-05-01 & 14:47:14 & 9386\\ \hline 
  4 & 15-05-03 & 10:21:57 & 14093\\ \hline 
  5 & 15-05-03 & 15:43:20 & 9875\\ \hline 
  6 & 15-05-04 & 09:42:39 & 10156\\ \hline 
  7 & 15-05-04 & 11:14:43 & 11933\\ \hline 
  8 & 15-05-04 & 17:42:34 & 10056\\ \hline 
  9 & 15-05-05 & 08:32:15 & 12216\\ \hline 
  10 & 15-05-05 & 08:55:02 & 9767\\ \hline 
  11 & 15-05-05 & 09:15:16 & 10153\\ \hline 
  12 & 15-05-05 & 10:12:57 & 6992\\ \hline 
  13 & 15-05-06 & 14:35:46 & 8703\\ \hline 
  14 & 15-05-06 & 17:01:37 & 10984\\ \hline 
  15 & 15-05-06 & 18:37:00 & 10186\\ \hline 
  \end{tabular}
\end{center}

Medelvärdet avrundat till närmsta heltal är: 10556 tester per sekund.

Standardavvikelsen avrundat till närmsta heltal är: 1710.

Den tid det tar från det att koden läggs upp på GitHub till dess att 
testkoden börjar köras är i genomsnitt ca en minut. Sedan tar det även ca 30
sekunder för alla node-moduler att installeras innan själva
testfallen börjar köras.

Undersökningen om vad om är svårt att testa ledde till följande resultat.
Det visade sig att vanliga Javascriptfunktioner är lätta
att testa så länge de ligger utanför jQueryfunktionen
\emph{\$(document).ready()}. Det kan illustreras med några
rader kod.

\begin{lstlisting}[
  basicstyle = \small
]
function easyToTest() {
  return 0;
}

$(document).ready(function() {

  $('#id1').click(
    var test = easyToTest(); 
  );

  $('#id2').click(
    //Hard to test
    var test = 0; 
  );
});

\end{lstlisting}

Det är alltså rekommenderat att skriva alla Javascriptfunktioner utanför 
jQueryfunktionen \emph{\$(document).ready()} eftersom koden då blir 
lättare att testa.

JQueryfunktioner är i allmänhet svårare att testa än vanliga
Javascriptfunktioner. Anledningen är att jQueryfunktioner
startas av och manipulerar HTML-objekt. Ett sätt att lösa detta är
att använda en egen \textunderscore Specrunner.html. Då kan HTML-objekten i den
filen användas för att testa jQueryfunktionerna.

\subsection{Diskussion}
Under den här rubriken diskuteras rapportens resultat
och metod.

\subsubsection{Resultat}
Resultatet angående hur effektivt det var att använda Travis CI
var inte särskilt förvånande. Att den lilla testfunktionen kunde köras
ca $10^4$ gånger per sekund verkar rimligt. Det intressanta med resultatet
var att det kan dröja ca en minut från att koden läggs upp på GitHub
till dess att testfallen börjar köras och att effektiviteten
kan variera med ca $\pm 30 \%$ vid olika tidpunkter.

Det är även värt att påpeka att hur koden skrivs och struktureras
i väldigt hög grad påverkar hur lätt den blir att testa. I alla fall
med den metod som använts under arbetet med den här rapporten. Det är 
därför en god idé att tidigt i ett projekt sätta upp riktlinjer för hur
koden ska skrivas på ett sätt som gör den lätt att testa. Då kan man
tjäna in tid som kanske annars hade behövt användas till att 
skriva om koden senare i projektet.

\subsubsection{Metod}
Den metod för testning som användes under arbetet med den här rapporten var
medvetet väldigt enkel och inte särskilt kraftfull. Anledningen var
att det var mycket annat som skulle göras i projektet och tiden var
begränsad till 300 timmar. I efterhand visade det sig dock att det hade varit 
möjligt att lägga lite mer tid på att använda en lite
mer avancerad metod för att kunna testa en större
del av koden i projektet. Nu användes den överblivna tiden istället till
att göra fler manuella tester av koden.

Det skulle inte vara några problem att skala metoden till ett större projekt.
Men i ett större projekt skulle det antagligen finnas någon som arbetar
uteslutande med testning och då skulle det finnas tid till att använda en
mer avancerad och mer kraftfull metod. Därför är det en bättre idé att 
använda den här metoden i ett mindre projekt.

Det är även värt att nämna ett en nackdel med metoden som använts för tester
under projektet
är att om ett eventuellt fel upptäcks så är koden redan upplagd på GitHub.
Det hade varit bättre om koden kunde testas innan den lades upp på GitHub
eftersom det då skulle gå att förebygga fel istället för att upptäcka 
dem i efterhand.

\subsection{Slutsatser}
Under den här rubriken presenteras svaren på
frågeställningarna och framtida arbete som skulle
kunna utföras inom det område som behandlats i rapporten.

\subsubsection{Hur kan man använda webbaserade tjänster för
att utföra kontinuerliga automatiserade tester av webbapplikationer?}
Ett sätt att använda en webbaserad tjänst för att utföra kontinuerliga 
automatiserade tester av en webbapplikation är att använda  
Travis CI tillsammans med Jasmine på det sätt som beskrivits
under avsnittet \emph{E.4 Metod}. Observera att den webbapplikation som testades
var skriven med Javascript. Node.js användes på serversidan och 
jQuery användes på klientsidan.

\subsubsection{Hur effektivt är det att använda en webbaserad tjänst
för automatiserade tester?}
Det enkla testfallet som användes kunde köras ca $10^4$ gånger per sekund och
det tar ca 60 sekunder från att koden läggs upp på GitHub till det att den
börjar köras på servern.

Om man använder ett antal testfall (med liknande komplexitet som
det som användes i den här rapporten) i storleksordningen $60 \cdot 10^4$ eller
mindre så kommer den största väntetiden vara tiden från att koden
läggs upp på GitHub till det att den börjar köras på servern. Själva
testfallen kommer att köras på mindre än en minut.

\subsubsection{Vilka typer av tester är svåra att utföra 
med en sådan tjänst?}
Kortfattat kan man säga att vanliga Javascriptfunktioner är
väldigt enkla att testa med de metoder, ramverk och tjänster som
använts under arbetet med den här rapporten. Men jQueryfunktioner och
Javascriptfunktioner i jQueryfunktioner är svårare att
testa. För en mer detaljerad beskrivning se rubriken \emph{E.5 Resultat}.

\subsubsection{Framtida arbete inom området}
Det finns mycket arbete som skulle kunna utföras inom området som behandlats
i den här rapporten.

Det vore intressant att undersöka hur stor andel av de totala felen
som upptäcks med den typ av testning som har beskrivits i den här rapporten.
Det kräver dock mer tid och resurser än vad som var tillgängligt under
arbetet den här rapporten.

Det vore även intressant att undersöka vilka nackdelar och fördelar det
finns med webbaserade testmiljöer jämfört med lokala
testmiljöer.

En annan undersökning som skulla kunna utföras är att jämföra
flera olika ramverk för testning av Javascript. Till exempel skulle
man kunna jämföra Jasmine och Mocha. Man skulle även kunna jämföra
flera olika webbaserade tjänster för automatiserade tester. Till
exempel skulle man kunna jämföra Travis CI med Codeship och Drone.io.

%\subsection{Referenser}
%\vspace{-9mm}
%\renewcommand{\refname}{}
%\begin{thebibliography}{9}

%\bibitem{Royce}
%W.W. Royce, ''Managing the development of large software systems,''
%\textit{Proceedings of IEEE WESCON}, pp. 2, aug, 1970.
%[Online].
%Tillgänglig (nytryckt med annan sidnumrering):
%\url{http://www.cs.umd.edu/class/spring2003/cmsc838p/Process/waterfall.pdf}.
%[Hämtad april 28, 2015].

%\bibitem{Karlsson}
%O. Karlsson, ''Automatiserad testning av webbapplikationer,''
%Linköpings univ., Linköping, Sverige, 2014, pp. 43.
%[Online]. 
%Tillgänglig: 
%\url{http://www.diva-portal.org/smash/get/diva2:727654/FULLTEXT01.pdf}.
%[Hämtad april 19, 2015].

%\end{thebibliography}
